[llm]
backend = "deepseek"           # default
model   = "deepseek/deepseek-chat"     # USANDO CODER (no chat), como pediste

[deepseek]
base_url = "https://api.deepseek.com"  # OJO: sin /v1

[ollama]
base_url = "http://localhost:11434/v1"
model    = "llama3.1"

[run]
max_retries = 5
timeout_sec = 120
save_plots  = true
